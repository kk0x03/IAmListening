# utils/whisper_asr.py

import whisper
import numpy as np

# åŠ è½½æ¨¡å‹
print("ğŸ¤– åŠ è½½ Whisper æ¨¡å‹...")
model = whisper.load_model("base", device="cpu")  # æˆ– "cuda" å¦‚æœæœ‰ GPU
print("âœ… Whisper æ¨¡å‹åŠ è½½å®Œæˆ")

def transcribe_audio(audio: np.ndarray, sample_rate=16000) -> str:
    """
    ç”¨ openai/whisper æ¨¡å‹è¯†åˆ« float32 çš„ waveform éŸ³é¢‘æ•°æ®
    è¦æ±‚: 1D float32 array, é‡‡æ ·ç‡ä¸º 16000
    è¿”å›: è¯†åˆ«å‡ºçš„æ–‡æœ¬
    """
    if sample_rate != 16000:
        raise ValueError("Whisper åªæ”¯æŒ 16000Hz é‡‡æ ·ç‡")

    # pad/trim åˆ° 30sï¼ˆwhisper è¦æ±‚ï¼‰
    audio = whisper.pad_or_trim(audio)

    # ç”Ÿæˆæ¢…å°”é¢‘è°±å›¾
    mel = whisper.log_mel_spectrogram(audio).to(model.device)

    # æ¨ç†
    options = whisper.DecodingOptions(language='zh', fp16=False)
    result = model.decode(mel, options)

    return result.text.strip()