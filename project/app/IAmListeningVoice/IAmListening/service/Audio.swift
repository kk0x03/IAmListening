import Foundation
import AVFoundation
import Speech
class Audio: ObservableObject {
    static let shared = Audio()

    @Published var result: String = "æœªå¼€å§‹"
    @Published var isRecording: Bool = false
    @Published var partialResult: String = ""  // æ–°å¢ï¼šå®æ—¶éƒ¨åˆ†è¯†åˆ«ç»“æœ
    @Published var finalResult: String = ""    // æ–°å¢ï¼šæœ€ç»ˆç¡®è®¤ç»“æœ

    private var yamnet = YAMNetService.shared
    private let llm = LLMInferenceService.shard
    private var audioEngine: AVAudioEngine!
    private var converter: AVAudioConverter?
    private var audioBuffer = [Float]()

    private let inputLength = 15600  // 0.975 ç§’ï¼ŒåŒ¹é… YAMNet è¾“å…¥

    private var speechSegmentBuffer = [Float]()  // è¯´è¯æ®µç¼“å­˜
    private var isSpeaking = false
    private var lastSpeechTime: Date?
    private var collecting = false
    private var forceStopTimer: Timer?

    private let silenceTimeout: TimeInterval = 1.5  // é™éŸ³è¶…æ—¶æ—¶é—´ï¼Œé¿å…æ­£å¸¸è¯´è¯ä¸­çš„çŸ­æš‚åœé¡¿
    private var lastASRUpdateTime: Date?  // è·Ÿè¸ª ASR ç»“æœæœ€åæ›´æ–°æ—¶é—´
    private var lastASRContent: String = ""  // è·Ÿè¸ªä¸Šæ¬¡ ASR å†…å®¹
    
    private let inferenceQueue = InferenceQueue()
    
    // æµå¼è¯­éŸ³è¯†åˆ«ç›¸å…³
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-CN"))!
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private var isStreamingASR = false

    // å¯åŠ¨æµå¼ASR
    private func startStreamingASR() {
        guard !isStreamingASR else { return }
        
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        recognitionRequest?.shouldReportPartialResults = true  // å…³é”®ï¼šå¯ç”¨éƒ¨åˆ†ç»“æœ
        recognitionRequest?.requiresOnDeviceRecognition = false
        
        guard let recognitionRequest = recognitionRequest else { return }
        
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] result, error in
            DispatchQueue.main.async {
                if let result = result {
                    let transcription = result.bestTranscription.formattedString
                    
                    if result.isFinal {
                        // æœ€ç»ˆç»“æœ
                        self?.finalResult = transcription
                        self?.partialResult = ""
                        
                        // ğŸ”¥ é‡ç½® ASR è·Ÿè¸ªçŠ¶æ€
                        self?.lastASRUpdateTime = nil
                        self?.lastASRContent = ""
                        
                        print("ğŸ¯ æœ€ç»ˆè¯†åˆ«ç»“æœ: \(transcription)")
                        
                        // è§¦å‘LLMå¤„ç†
                        Task {
                            await self?.llm.generateBlockingResponse(for: transcription)
                        }
                        
                        // é‡å¯è¯†åˆ«ä»¥ç»§ç»­ç›‘å¬
                        DispatchQueue.main.asyncAfter(deadline: .now() + 0.2) {
                            self?.restartStreamingASR()
                        }
                    } else {
                        // å®æ—¶éƒ¨åˆ†ç»“æœ
                        self?.partialResult = transcription
                        
                        // ğŸ”¥ è·Ÿè¸ª ASR å†…å®¹å˜åŒ–
                        if transcription != self?.lastASRContent {
                            self?.lastASRUpdateTime = Date()
                            self?.lastASRContent = transcription
                        }
                        
                        print("âš¡ å®æ—¶è¯†åˆ«: \(transcription)")
                    }
                }
                
                if error != nil {
                    self?.stopStreamingASR()
                }
            }
        }
        
        isStreamingASR = true
    }
    
    // åœæ­¢æµå¼ASR
    private func stopStreamingASR() {
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        recognitionRequest = nil
        recognitionTask = nil
        isStreamingASR = false
    }
    
    // å®Œæˆå½“å‰è¯­éŸ³è¯†åˆ«å¹¶é‡æ–°å¼€å§‹æ–°çš„è¯†åˆ«ä¼šè¯
    private func finalizeSpeechRecognition() {
        guard isStreamingASR else { return }
        forceStopTimer?.invalidate()
        recognitionRequest?.endAudio()
        // ä¸è¦ç«‹å³ cancel recognitionTaskï¼Œç­‰å¾… isFinal å›è°ƒ
        // ç”± recognitionTask çš„å›è°ƒä¸­ isFinal è§¦å‘åå†é‡å¯è¯†åˆ«
    }
    
    // é‡æ–°å¯åŠ¨æµå¼ASR
    private func restartStreamingASR() {
        // å…ˆåœæ­¢å½“å‰çš„è¯†åˆ«
        recognitionTask?.cancel()
        recognitionRequest = nil
        recognitionTask = nil
        isStreamingASR = false
        
        // ğŸ”¥ é‡ç½® ASR è·Ÿè¸ªçŠ¶æ€ï¼Œå‡†å¤‡æ¥å—æ–°çš„è¯­éŸ³è¾“å…¥
        lastASRUpdateTime = nil
        lastASRContent = ""
        partialResult = ""
        
        // é‡æ–°å¼€å§‹è¯†åˆ«
        startStreamingASR()
        
        // ğŸ”¥ å…³é”®ä¿®å¤ï¼šé‡æ–°å¯åŠ¨å…œåº•å®šæ—¶å™¨
        forceStopTimer?.invalidate()
        forceStopTimer = Timer.scheduledTimer(withTimeInterval: 8.0, repeats: false) { _ in
            print("â° 8 ç§’å…œåº•å®šæ—¶å™¨è§¦å‘ï¼Œå¼ºåˆ¶é‡å¯è¯†åˆ«")
            // ğŸ”¥ å…³é”®ä¿®å¤ï¼šç›´æ¥é‡å¯è¯†åˆ«ï¼Œä¸ä¾èµ– isFinal å›è°ƒ
            self.restartStreamingASR()
        }
    }

    private func startAudioEngine() {
        let audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession.setCategory(.playAndRecord, options: .defaultToSpeaker)
            try audioSession.setMode(.measurement)
            try audioSession.setPreferredSampleRate(44100)
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        } catch {
            print("éŸ³é¢‘ä¼šè¯é…ç½®å¤±è´¥: \(error)")
            return
        }

        audioEngine = AVAudioEngine()
        let inputNode = audioEngine.inputNode
        let inputFormat = inputNode.inputFormat(forBus: 0)
        let targetFormat = AVAudioFormat(commonFormat: .pcmFormatFloat32,
                                         sampleRate: 16000,
                                         channels: 1,
                                         interleaved: false)!
        self.converter = AVAudioConverter(from: inputFormat, to: targetFormat)
        
        // å¯åŠ¨æµå¼ASR
        startStreamingASR()
        
        var preSpeechBuffer: [Float] = []
        let preSpeechLength = 16000 * 2  // 2 ç§’é¢„å½•

        inputNode.installTap(onBus: 0, bufferSize: 4096, format: inputFormat) { buffer, _ in
                guard let converter = self.converter,
                      let newBuffer = AVAudioPCMBuffer(pcmFormat: targetFormat, frameCapacity: 4096) else { return }

                var error: NSError? = nil
                let inputBlock: AVAudioConverterInputBlock = { _, outStatus in
                    outStatus.pointee = .haveData
                    return buffer
                }

                converter.convert(to: newBuffer, error: &error, withInputFrom: inputBlock)

                if let err = error {
                    print("è½¬æ¢å¤±è´¥: \(err)")
                    return
                }

                guard let channelData = newBuffer.floatChannelData?[0] else { return }
                let frameCount = Int(newBuffer.frameLength)

                // å¢ç›Šå¤„ç† + é™å™ª + é™å¹…
                let gain: Float = 1.5
                let noiseThreshold: Float = 0.02  // é™å™ªé˜ˆå€¼
                let segment = (0..<frameCount).map { i in
                    let sample = channelData[i] * gain
                    // ç®€å•é™å™ªï¼šä½äºé˜ˆå€¼çš„ä¿¡å·è§†ä¸ºå™ªéŸ³ï¼Œè¡°å‡å¤„ç†
                    let denoisedSample = abs(sample) < noiseThreshold ? sample * 0.1 : sample
                    return max(-1.0, min(denoisedSample, 1.0))
                }
                
                // è®¡ç®—å½“å‰éŸ³é¢‘æ®µçš„éŸ³é‡ï¼ˆRMSï¼‰
                let rms = sqrt(segment.map { $0 * $0 }.reduce(0, +) / Float(segment.count))
                let volumeThreshold: Float = 0.03  // éŸ³é‡é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è®¤ä¸ºæ˜¯é™éŸ³

                // ğŸ”¥ å…³é”®ä¼˜åŒ–ï¼šåŒæ—¶å‘é€éŸ³é¢‘åˆ°æµå¼ASR
                if self.isStreamingASR, let recognitionRequest = self.recognitionRequest {
                    // åˆ›å»ºç”¨äºASRçš„éŸ³é¢‘ç¼“å†²åŒºï¼ˆä½¿ç”¨åŸå§‹è¾“å…¥æ ¼å¼ï¼‰
                    recognitionRequest.append(buffer)
                }

                // 1. ç´¯ç§¯å®æ—¶ä¸» buffer
                self.audioBuffer.append(contentsOf: segment)

            if !self.collecting {
                // 2. ç»´æŠ¤ 2 ç§’é¢„å½•ç¼“å­˜
                preSpeechBuffer.append(contentsOf: segment)
                if preSpeechBuffer.count > preSpeechLength {
                    preSpeechBuffer.removeFirst(preSpeechBuffer.count - preSpeechLength)
                }
            }
                

                // 3. åˆ†ç±»é€»è¾‘è§¦å‘
                while self.audioBuffer.count >= self.inputLength {
                    let segment = Array(self.audioBuffer.prefix(self.inputLength))
                    self.audioBuffer.removeFirst(self.inputLength)

                    self.yamnet.runModel(audioData: segment)
                    let label = self.yamnet.classify.trimmingCharacters(in: .whitespacesAndNewlines)
                    print("ğŸ” å½“å‰å¸§åˆ†ç±»ï¼š[\(label)]")

                    // ğŸ¯ æ”¹è¿›çš„è¯­éŸ³æ£€æµ‹ï¼šä¼˜å…ˆä½¿ç”¨å®æ—¶è¯†åˆ«ç»“æœï¼Œè¾…ä»¥åˆ†ç±»å’ŒéŸ³é‡åˆ¤æ–­
                    let isSpeechClassified = label.lowercased().contains("speech") || 
                                           label.lowercased().contains("conversation") ||
                                           label.lowercased().contains("narration") ||
                                           label.lowercased().contains("monologue")
                    let hasValidVolume = rms > volumeThreshold
                    
                    // ğŸ”¥ å…³é”®æ”¹è¿›ï¼šå¦‚æœå®æ—¶è¯†åˆ«æœ‰è¾“å‡ºä¸”ä¸ä¸ºç©ºï¼Œå°±è®¤ä¸ºæ˜¯æœ‰æ•ˆè¯­éŸ³
                    let hasASROutput = !self.partialResult.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty
                    
                    // ğŸ”¥ æ£€æŸ¥ ASR å†…å®¹æ˜¯å¦è¶…è¿‡2ç§’æ²¡æœ‰æ›´æ–°
                    let isASRStale: Bool
                    if let lastUpdateTime = self.lastASRUpdateTime {
                        isASRStale = Date().timeIntervalSince(lastUpdateTime) > 2.0
                    } else {
                        isASRStale = false
                    }
                    
                    // ğŸ”¥ å…³é”®ä¿®å¤ï¼šç»¼åˆå¤šä¸ªæ¡ä»¶åˆ¤æ–­æœ‰æ•ˆè¯­éŸ³
                    let isSilenceClassified = label.lowercased().contains("silence")
                    let isVeryLowVolume = rms < 0.001  // æä½éŸ³é‡é˜ˆå€¼
                    
                    let isValidSpeech: Bool
                    if isSilenceClassified && isVeryLowVolume {
                        // æ˜ç¡®çš„é™éŸ³çŠ¶æ€ï¼šåˆ†ç±»ä¸º Silence ä¸”éŸ³é‡æä½
                        isValidSpeech = false
                    } else if hasASROutput && isASRStale {
                        // ASR æœ‰è¾“å‡ºä½†è¶…è¿‡2ç§’æ²¡æœ‰æ›´æ–°ï¼Œè®¤ä¸ºæ˜¯æ— æ•ˆè¯­éŸ³
                        isValidSpeech = false
                    } else {
                        // å…¶ä»–æƒ…å†µï¼šä¼˜å…ˆä½¿ç”¨ ASR è¾“å‡ºï¼Œè¾…ä»¥åˆ†ç±»å’ŒéŸ³é‡åˆ¤æ–­
                        isValidSpeech = hasASROutput || (isSpeechClassified && hasValidVolume)
                    }
                    
                    let asrStaleInfo = isASRStale ? "(è¶…è¿‡2ç§’æœªæ›´æ–°)" : ""
                     print("ğŸ” åˆ†ç±»: \(label), éŸ³é‡: \(String(format: "%.4f", rms)), ASRè¾“å‡º: [\(self.partialResult)]\(asrStaleInfo), æœ‰æ•ˆè¯­éŸ³: \(isValidSpeech)")
                    
                    if isValidSpeech {
                        if !self.collecting {
                            self.collecting = true
                            print("ğŸ¤ æ£€æµ‹åˆ°æœ‰æ•ˆè¯­éŸ³å¼€å§‹ (åˆ†ç±»: \(label), éŸ³é‡: \(String(format: "%.4f", rms)))")
                        }
                        self.lastSpeechTime = Date()
                        
                        // ğŸ”¥ é‡ç½®å…œåº•å®šæ—¶å™¨ï¼šæ£€æµ‹åˆ°è¯­éŸ³æ—¶é‡æ–°è®¡æ—¶
                        self.forceStopTimer?.invalidate()
                        self.forceStopTimer = Timer.scheduledTimer(withTimeInterval: 8.0, repeats: false) { _ in
                            print("â° 8 ç§’å…œåº•å®šæ—¶å™¨è§¦å‘ï¼Œå¼ºåˆ¶é‡å¯è¯†åˆ«")
                            // ğŸ”¥ å…³é”®ä¿®å¤ï¼šç›´æ¥é‡å¯è¯†åˆ«ï¼Œä¸ä¾èµ– isFinal å›è°ƒ
                            self.restartStreamingASR()
                        }
                    } else {
                        // åªæœ‰åœ¨ç¡®å®æ£€æµ‹åˆ°è¯­éŸ³åæ‰è€ƒè™‘é™éŸ³è¶…æ—¶
                        if self.collecting,
                           let lastTime = self.lastSpeechTime,
                           Date().timeIntervalSince(lastTime) >= self.silenceTimeout {
                            self.collecting = false
                            self.lastSpeechTime = nil
                            print("ğŸ”‡ è¯­éŸ³æ®µç»“æŸï¼Œè§¦å‘æœ€ç»ˆè¯†åˆ« (é™éŸ³æ—¶é•¿: \(String(format: "%.1f", Date().timeIntervalSince(lastTime)))ç§’)")
                            
                            // ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¸»åŠ¨ç»“æŸå½“å‰è¯†åˆ«è¯·æ±‚ä»¥è·å¾—æœ€ç»ˆç»“æœ
                            self.finalizeSpeechRecognition()
                        }
                    }
                }
            }
        do {
            try audioEngine.start()
            print("ğŸ¤ Audio engine started")
            
            // å¯åŠ¨æµå¼ASRè¿›è¡Œå®æ—¶è¯†åˆ«
            startStreamingASR()
            
            // å¯åŠ¨8ç§’å…œåº•å®šæ—¶å™¨
            forceStopTimer = Timer.scheduledTimer(withTimeInterval: 8.0, repeats: false) { _ in
                print("â° 8 ç§’å…œåº•å®šæ—¶å™¨è§¦å‘ï¼Œå¼ºåˆ¶é‡å¯è¯†åˆ«")
                // ğŸ”¥ å…³é”®ä¿®å¤ï¼šç›´æ¥é‡å¯è¯†åˆ«ï¼Œä¸ä¾èµ– isFinal å›è°ƒ
                self.restartStreamingASR()
            }
        } catch {
            print("éŸ³é¢‘å¼•æ“å¯åŠ¨å¤±è´¥: \(error)")
        }
    }

    func toggle() {
        if isRecording {
            audioEngine?.stop()
            audioEngine?.inputNode.removeTap(onBus: 0)
            stopStreamingASR()  // åœæ­¢æµå¼ASR
            isRecording = false
            
            // åªæ¸…ç©ºå®æ—¶ç»“æœï¼Œä¿ç•™æœ€ç»ˆç»“æœä¾›ç”¨æˆ·æŸ¥çœ‹
            partialResult = ""
            // ä¸æ¸…ç©º finalResultï¼Œè®©ç”¨æˆ·èƒ½çœ‹åˆ°æœ€åçš„è¯†åˆ«ç»“æœ
        } else {
            start()
        }
    }

    private func start() {
        // è¯·æ±‚è¯­éŸ³è¯†åˆ«æƒé™
        SFSpeechRecognizer.requestAuthorization { authStatus in
            guard authStatus == .authorized else {
                print("è¯­éŸ³è¯†åˆ«æƒé™æœªæˆäºˆ")
                return
            }
            
            // è¯·æ±‚éº¦å…‹é£æƒé™
            AVAudioApplication.requestRecordPermission { granted in
                guard granted else {
                    print("éº¦å…‹é£æƒé™æœªå¼€å¯")
                    return
                }
                DispatchQueue.main.async {
                    self.startAudioEngine()
                    self.isRecording = true
                }
            }
        }
    }
}

actor InferenceQueue {
    func enqueue(buffer: [Float]) async {
        await WhisperState.shared.transcribeBuffer(buffer)
        await LLMInferenceService.shard.generateBlockingResponse(for: WhisperState.shared.messageLog)
    }
}
